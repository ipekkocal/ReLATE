{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CASE 1\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from tsai.all import *\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "def training_loss(pred, target):\n",
    "    return F.cross_entropy(pred, target)\n",
    "\n",
    "def train_and_extract_embeddings_in_batches(dsid, hidden_size, device, batch_size=16, save_path='train_embeddings.npy', val_save_path='val_embeddings.npy'):\n",
    "\n",
    "    X_train, y_train, X_test, y_test = get_UCR_data(dsid, return_split=True)\n",
    "\n",
    "    X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train, test_size=0.2, shuffle=False)\n",
    "\n",
    "    print(f\"X_train_split shape: {X_train_split.shape}, X_val_split shape: {X_val_split.shape}\")\n",
    "\n",
    "    tfms = [None, [Categorize()]]  \n",
    "    dsets_train = TSDatasets(X_train_split, y_train_split, tfms=tfms, splits=None, inplace=True)\n",
    "    #dsets_val = TSDatasets(X_val_split, y_train_split, tfms=tfms, splits=None, inplace=True)\n",
    "    dls_train = TSDataLoaders.from_dsets(dsets_train.train, dsets_train.train, bs=batch_size, batch_tfms=[TSStandardize()], num_workers=0)\n",
    "    #dls_val = TSDataLoaders.from_dsets(dsets_val.train, dsets_val.train, bs=batch_size, batch_tfms=[TSStandardize()], num_workers=0)\n",
    "\n",
    "    c_in = dls_train.vars \n",
    "    c_out = dls_train.c \n",
    "\n",
    "    model = CustomCNN(c_in, c_out).to(device)\n",
    "    learn = Learner(dls_train, model, loss_func=training_loss, metrics=accuracy) \n",
    "    learn.fit_one_cycle(25, 0.002)  \n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    def extract_embeddings(X_split, save_path):\n",
    "        X_split_tensor = torch.tensor(X_split).float().to(device)\n",
    "        print(f\"X_split_tensor shape before batching: {X_split_tensor.shape}\")\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(X_split_tensor), batch_size):\n",
    "            batch_X = X_split_tensor[i:i+batch_size]\n",
    "            print(f\"Batch_X shape before CNN: {batch_X.shape}\")\n",
    "            with torch.no_grad():\n",
    "                _, batch_embeddings = model(batch_X, return_embeddings=True)  # Extract embeddings\n",
    "            all_embeddings.append(batch_embeddings.cpu().numpy())\n",
    "        all_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "        np.save(save_path, all_embeddings)\n",
    "\n",
    "    extract_embeddings(X_train_split, save_path)\n",
    "\n",
    "    extract_embeddings(X_val_split, val_save_path)\n",
    "\n",
    "dataset_ids = ['Libras', 'RacketSports', 'NATOPS', 'UWaveGestureLibrary', 'Cricket', 'Ering', 'BasicMotions', 'Epilepsy']\n",
    "hidden_size = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Ensure the correct device (GPU or CPU)\n",
    "\n",
    "train_embeddings_dict = {}\n",
    "val_embeddings_dict = {}\n",
    "\n",
    "for dsid in dataset_ids:\n",
    "    train_save_path = f'train_embeddings_{dsid}.npy'\n",
    "    val_save_path = f'val_embeddings_{dsid}.npy'\n",
    "    train_and_extract_embeddings_in_batches(dsid, hidden_size, device, batch_size=16, save_path=train_save_path, val_save_path=val_save_path)\n",
    "\n",
    "    train_embeddings_dict[dsid] = np.load(train_save_path)\n",
    "    val_embeddings_dict[dsid] = np.load(val_save_path)\n",
    "\n",
    "normalized_train_embeddings_dict = {dsid: normalize(train_embeddings_dict[dsid], norm='l2') for dsid in dataset_ids}\n",
    "normalized_val_embeddings_dict = {dsid: normalize(val_embeddings_dict[dsid], norm='l2') for dsid in dataset_ids}\n",
    "\n",
    "for dsid1, dsid2 in itertools.combinations(dataset_ids, 2):\n",
    "\n",
    "    train_embeddings1_normalized = normalized_train_embeddings_dict[dsid1]\n",
    "    train_embeddings2_normalized = normalized_train_embeddings_dict[dsid2]\n",
    "    train_similarity_matrix = cosine_similarity(train_embeddings1_normalized, train_embeddings2_normalized)\n",
    "    avg_train_similarity = np.mean(train_similarity_matrix)\n",
    "    print(f\"Average train set similarity between {dsid1} and {dsid2}: {avg_train_similarity}\")\n",
    "\n",
    "    val_embeddings1_normalized = normalized_val_embeddings_dict[dsid1]\n",
    "    val_embeddings2_normalized = normalized_val_embeddings_dict[dsid2]\n",
    "    val_similarity_matrix = cosine_similarity(val_embeddings1_normalized, val_embeddings2_normalized)\n",
    "    avg_val_similarity = np.mean(val_similarity_matrix)\n",
    "    print(f\"Average validation set similarity between {dsid1} and {dsid2}: {avg_val_similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CASE 2 \n",
    "\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from tsai.all import *\n",
    "import numpy as np\n",
    "from art.attacks.evasion import (\n",
    "    BasicIterativeMethod, DeepFool, CarliniL2Method, MomentumIterativeMethod,\n",
    "    ElasticNet, AutoProjectedGradientDescent, FastGradientMethod, ZooAttack, BoundaryAttack\n",
    ")\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Random number generator to choose the attack (0-9)\n",
    "def random_attack_choice():\n",
    "    return random.randint(0, 9)\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_filters=64, kernel_size=3, dropout=0.5):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=num_filters, kernel_size=kernel_size)\n",
    "        self.conv2 = nn.Conv1d(in_channels=num_filters, out_channels=num_filters, kernel_size=kernel_size)\n",
    "        self.fc1 = nn.Linear(num_filters, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, return_embeddings=False):\n",
    "        x = x.permute(0, 1, 2)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.adaptive_max_pool1d(x, 1).squeeze(2)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        final_output = self.fc1(x)\n",
    "\n",
    "        if return_embeddings:\n",
    "            embeddings = x\n",
    "            return final_output, embeddings\n",
    "        else:\n",
    "            return final_output\n",
    "\n",
    "def training_loss(pred, target):\n",
    "    return F.cross_entropy(pred, target)\n",
    "\n",
    "def create_art_classifier(model, input_shape, nb_classes, device):\n",
    "    classifier = PyTorchClassifier(\n",
    "        model=model,\n",
    "        loss=training_loss,\n",
    "        input_shape=input_shape,\n",
    "        nb_classes=nb_classes,\n",
    "        optimizer=torch.optim.Adam(model.parameters(), lr=0.002),\n",
    "        device_type=device.type\n",
    "    )\n",
    "    return classifier\n",
    "\n",
    "def apply_attack(X, classifier, attack_number):\n",
    "    attack_number = 8\n",
    "    if attack_number == 0:\n",
    "        attack = BasicIterativeMethod(estimator=classifier, eps=0.1)\n",
    "    elif attack_number == 1:\n",
    "        attack = DeepFool(classifier=classifier)\n",
    "    elif attack_number == 2:\n",
    "        attack = CarliniL2Method(classifier=classifier)\n",
    "    elif attack_number == 3:\n",
    "        attack = MomentumIterativeMethod(estimator=classifier, eps=0.1)\n",
    "    elif attack_number == 4:\n",
    "        attack = ElasticNet(classifier=classifier)\n",
    "    elif attack_number == 5:\n",
    "        attack = AutoProjectedGradientDescent(estimator=classifier, eps=0.1)\n",
    "    elif attack_number == 6:\n",
    "        attack = FastGradientMethod(estimator=classifier, eps=0.1)\n",
    "    elif attack_number == 7:\n",
    "        attack = ZooAttack(classifier=classifier, confidence=0.5, targeted=False)\n",
    "    elif attack_number == 8:\n",
    "        attack = BoundaryAttack(estimator=classifier, targeted=False)\n",
    "    else:\n",
    "        return X\n",
    "    return attack.generate(X)\n",
    "\n",
    "def train_and_attack_validation(dsid, hidden_size, device, attack_number, batch_size=16, save_path='train_embeddings.npy', attacked_val_save_path='attacked_val_embeddings.npy'):\n",
    "\n",
    "    X_train, y_train, X_test, y_test = get_UCR_data(dsid, return_split=True)\n",
    "\n",
    "    #X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train, test_size=0.4, stratify=y_train, shuffle=False)\n",
    "    X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train, test_size=0.2, shuffle=False)\n",
    "    print(f\"X_train_split shape: {X_train_split.shape}, X_val_split shape: {X_val_split.shape}\")\n",
    "\n",
    "    tfms = [None, [Categorize()]]\n",
    "    dsets_train = TSDatasets(X_train_split, y_train_split, tfms=tfms, splits=None, inplace=True)\n",
    "    dls_train = TSDataLoaders.from_dsets(dsets_train.train, dsets_train.train, bs=batch_size,shuffle=False,batch_tfms=[TSStandardize()], num_workers=0)\n",
    "\n",
    "    c_in = dls_train.vars\n",
    "    c_out = dls_train.c\n",
    "\n",
    "    model = CustomCNN(c_in, c_out).to(device)\n",
    "    learn = Learner(dls_train, model, loss_func=training_loss, metrics=accuracy)\n",
    "    learn.fit_one_cycle(25, 0.002)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    classifier = create_art_classifier(model, input_shape=(c_in, X_train_split.shape[2]), nb_classes=c_out, device=device)\n",
    "\n",
    "    print(f\"Applying attack {attack_number} to the validation set\")\n",
    "    attacked_X_val_split = apply_attack(X_val_split, classifier, attack_number)\n",
    "\n",
    "    def extract_embeddings(X_split, save_path):\n",
    "        X_split_tensor = torch.tensor(X_split).float().to(device)\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(X_split_tensor), batch_size):\n",
    "            batch_X = X_split_tensor[i:i+batch_size]\n",
    "            with torch.no_grad():\n",
    "                _, batch_embeddings = model(batch_X, return_embeddings=True)\n",
    "            all_embeddings.append(batch_embeddings.cpu().numpy())\n",
    "        all_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "        np.save(save_path, all_embeddings)\n",
    "\n",
    "    extract_embeddings(X_train_split, save_path)\n",
    "\n",
    "    extract_embeddings(attacked_X_val_split, attacked_val_save_path)\n",
    "\n",
    "dataset_ids = ['RacketSports', 'NATOPS', 'UWaveGestureLibrary', 'Cricket', 'Ering', 'BasicMotions', 'Epilepsy']\n",
    "hidden_size = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "attack_number = 2\n",
    "print(f\"Chosen attack number: {attack_number}\")\n",
    "\n",
    "train_embeddings_dict = {}\n",
    "attacked_val_embeddings_dict = {}\n",
    "\n",
    "for dsid in dataset_ids:\n",
    "    train_save_path = f'train_embeddings_{dsid}.npy'\n",
    "    attacked_val_save_path = f'attacked_val_embeddings_{dsid}.npy'\n",
    "    train_and_attack_validation(dsid, hidden_size, device, attack_number, batch_size=16, save_path=train_save_path, attacked_val_save_path=attacked_val_save_path)\n",
    "\n",
    "    train_embeddings_dict[dsid] = np.load(train_save_path)\n",
    "    attacked_val_embeddings_dict[dsid] = np.load(attacked_val_save_path)\n",
    "\n",
    "normalized_train_embeddings_dict = {dsid: normalize(train_embeddings_dict[dsid], norm='l2') for dsid in dataset_ids}\n",
    "normalized_attacked_val_embeddings_dict = {dsid: normalize(attacked_val_embeddings_dict[dsid], norm='l2') for dsid in dataset_ids}\n",
    "\n",
    "for dsid1, dsid2 in itertools.combinations(dataset_ids, 2):\n",
    "\n",
    "    train_embeddings1_normalized = normalized_train_embeddings_dict[dsid1]\n",
    "    train_embeddings2_normalized = normalized_train_embeddings_dict[dsid2]\n",
    "    train_similarity_matrix = cosine_similarity(train_embeddings1_normalized, train_embeddings2_normalized)\n",
    "    avg_train_similarity = np.mean(train_similarity_matrix)\n",
    "    print(f\"Average train set similarity between {dsid1} and {dsid2}: {avg_train_similarity}\")\n",
    "\n",
    "    attacked_val_embeddings1_normalized = normalized_attacked_val_embeddings_dict[dsid1]\n",
    "    attacked_val_embeddings2_normalized = normalized_attacked_val_embeddings_dict[dsid2]\n",
    "    attacked_val_similarity_matrix = cosine_similarity(attacked_val_embeddings1_normalized, attacked_val_embeddings2_normalized)\n",
    "    avg_attacked_val_similarity = np.mean(attacked_val_similarity_matrix)\n",
    "    print(f\"Average attacked validation set similarity between {dsid1} and {dsid2}: {avg_attacked_val_similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CASE 3\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from tsai.all import *\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from art.attacks.evasion import (\n",
    "    BasicIterativeMethod, DeepFool, CarliniL2Method, MomentumIterativeMethod,\n",
    "    ElasticNet, AutoProjectedGradientDescent, FastGradientMethod, ZooAttack, BoundaryAttack\n",
    ")\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_filters=64, kernel_size=3, dropout=0.5):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=num_filters, kernel_size=kernel_size)\n",
    "        self.conv2 = nn.Conv1d(in_channels=num_filters, out_channels=num_filters, kernel_size=kernel_size)\n",
    "        self.fc1 = nn.Linear(num_filters, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, return_embeddings=False):\n",
    "        x = x.permute(0, 1, 2)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.adaptive_max_pool1d(x, 1).squeeze(2)\n",
    "        x = self.dropout(x)\n",
    "        final_output = self.fc1(x)\n",
    "\n",
    "        if return_embeddings:\n",
    "            embeddings = x\n",
    "            return final_output, embeddings\n",
    "        else:\n",
    "            return final_output\n",
    "\n",
    "def training_loss(pred, target):\n",
    "    return F.cross_entropy(pred, target)\n",
    "\n",
    "def create_art_classifier(model, input_shape, nb_classes, device):\n",
    "    classifier = PyTorchClassifier(\n",
    "        model=model,\n",
    "        loss=training_loss,\n",
    "        input_shape=input_shape,\n",
    "        nb_classes=nb_classes,\n",
    "        optimizer=torch.optim.Adam(model.parameters(), lr=0.002),\n",
    "        device_type=device.type\n",
    "    )\n",
    "    return classifier\n",
    "\n",
    "def apply_attack(X, classifier, attack_number):\n",
    "    if attack_number == 0:\n",
    "        attack = BasicIterativeMethod(estimator=classifier, eps=0.1)\n",
    "    elif attack_number == 1:\n",
    "        attack = DeepFool(classifier=classifier)\n",
    "    elif attack_number == 2:\n",
    "        attack = CarliniL2Method(classifier=classifier)\n",
    "    elif attack_number == 3:\n",
    "        attack = MomentumIterativeMethod(estimator=classifier, eps=0.1)\n",
    "    elif attack_number == 4:\n",
    "        attack = ElasticNet(classifier=classifier)\n",
    "    elif attack_number == 5:\n",
    "        attack = AutoProjectedGradientDescent(estimator=classifier, eps=0.1)\n",
    "    elif attack_number == 6:\n",
    "        attack = FastGradientMethod(estimator=classifier, eps=0.1)\n",
    "    elif attack_number == 7:\n",
    "        attack = ZooAttack(classifier=classifier, confidence=0.5, targeted=False)\n",
    "    elif attack_number == 8:\n",
    "        attack = BoundaryAttack(estimator=classifier, targeted=False)\n",
    "    else:\n",
    "        return X\n",
    "    return attack.generate(X)\n",
    "\n",
    "def apply_attack_decision(X_split, classifier, attack_decisions):\n",
    "    attacked_pieces = []\n",
    "    for piece, (attack_flag, attack_number) in zip(X_split, attack_decisions):\n",
    "        if attack_flag == 1:\n",
    "            attacked_piece = apply_attack(piece, classifier, attack_number)\n",
    "        else:\n",
    "            attacked_piece = piece\n",
    "        attacked_pieces.append(attacked_piece)\n",
    "    return np.concatenate(attacked_pieces, axis=0)\n",
    "\n",
    "def split_data(X, num_pieces=5):\n",
    "    splits = np.array_split(X, num_pieces)\n",
    "    return splits\n",
    "\n",
    "def generate_attack_decisions(num_pieces=5):\n",
    "    attack_decisions = []\n",
    "    for _ in range(num_pieces):\n",
    "        attack_flag = random.randint(0, 1)\n",
    "        attack_number = random.randint(0, 9) if attack_flag == 1 else None\n",
    "        attack_decisions.append((attack_flag, attack_number))\n",
    "    return attack_decisions\n",
    "\n",
    "def train_and_attack_validation(dsid, hidden_size, device, attack_decisions, batch_size=16, save_path='train_embeddings.npy', attacked_val_save_path='attacked_val_embeddings.npy'):\n",
    "\n",
    "    X_train, y_train, X_test, y_test = get_UCR_data(dsid, return_split=True)\n",
    "\n",
    "    X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train, test_size=0.2, shuffle=False)\n",
    "\n",
    "    print(f\"X_train_split shape: {X_train_split.shape}, X_val_split shape: {X_val_split.shape}\")\n",
    "\n",
    "    tfms = [None, [Categorize()]]\n",
    "    dsets_train = TSDatasets(X_train_split, y_train_split, tfms=tfms, splits=None, inplace=True)\n",
    "    dls_train = TSDataLoaders.from_dsets(dsets_train.train, dsets_train.train, bs=batch_size,shuffle=False, batch_tfms=[TSStandardize()], num_workers=0)\n",
    "\n",
    "    c_in = dls_train.vars\n",
    "    c_out = dls_train.c\n",
    "\n",
    "    model = CustomCNN(c_in, c_out).to(device)\n",
    "    learn = Learner(dls_train, model, loss_func=training_loss, metrics=accuracy)\n",
    "    learn.fit_one_cycle(25, 0.002)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    classifier = create_art_classifier(model, input_shape=(c_in, X_train_split.shape[2]), nb_classes=c_out, device=device)\n",
    "\n",
    "    X_val_pieces = split_data(X_val_split, num_pieces=5)\n",
    "\n",
    "    attacked_X_val_split = apply_attack_decision(X_val_pieces, classifier, attack_decisions)\n",
    "\n",
    "    def extract_embeddings(X_split, save_path):\n",
    "        X_split_tensor = torch.tensor(X_split).float().to(device)\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(X_split_tensor), batch_size):\n",
    "            batch_X = X_split_tensor[i:i+batch_size]\n",
    "            with torch.no_grad():\n",
    "                _, batch_embeddings = model(batch_X, return_embeddings=True)\n",
    "            all_embeddings.append(batch_embeddings.cpu().numpy())\n",
    "        all_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "        np.save(save_path, all_embeddings)\n",
    "\n",
    "    extract_embeddings(X_train_split, save_path)\n",
    "\n",
    "    extract_embeddings(attacked_X_val_split, attacked_val_save_path)\n",
    "\n",
    "dataset_ids = ['Libras', 'RacketSports', 'NATOPS', 'UWaveGestureLibrary', 'Cricket', 'Ering', 'BasicMotions', 'Epilepsy']\n",
    "hidden_size = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "attack_decisions = generate_attack_decisions(num_pieces=5)\n",
    "print(f\"Attack decisions for each piece: {attack_decisions}\")\n",
    "\n",
    "train_embeddings_dict = {}\n",
    "attacked_val_embeddings_dict = {}\n",
    "\n",
    "for dsid in dataset_ids:\n",
    "    train_save_path = f'train_embeddings_{dsid}.npy'\n",
    "    attacked_val_save_path = f'attacked_val_embeddings_{dsid}.npy'\n",
    "    train_and_attack_validation(dsid, hidden_size, device, attack_decisions, batch_size=16, save_path=train_save_path, attacked_val_save_path=attacked_val_save_path)\n",
    "\n",
    "    train_embeddings_dict[dsid] = np.load(train_save_path)\n",
    "    attacked_val_embeddings_dict[dsid] = np.load(attacked_val_save_path)\n",
    "\n",
    "normalized_train_embeddings_dict = {dsid: normalize(train_embeddings_dict[dsid], norm='l2') for dsid in dataset_ids}\n",
    "normalized_attacked_val_embeddings_dict = {dsid: normalize(attacked_val_embeddings_dict[dsid], norm='l2') for dsid in dataset_ids}\n",
    "\n",
    "for dsid1, dsid2 in itertools.combinations(dataset_ids, 2):\n",
    "\n",
    "    train_embeddings1_normalized = normalized_train_embeddings_dict[dsid1]\n",
    "    train_embeddings2_normalized = normalized_train_embeddings_dict[dsid2]\n",
    "    train_similarity_matrix = cosine_similarity(train_embeddings1_normalized, train_embeddings2_normalized)\n",
    "    avg_train_similarity = np.mean(train_similarity_matrix)\n",
    "    print(f\"Average train set similarity between {dsid1} and {dsid2}: {avg_train_similarity}\")\n",
    "\n",
    "    attacked_val_embeddings1_normalized = normalized_attacked_val_embeddings_dict[dsid1]\n",
    "    attacked_val_embeddings2_normalized = normalized_attacked_val_embeddings_dict[dsid2]\n",
    "    attacked_val_similarity_matrix = cosine_similarity(attacked_val_embeddings1_normalized, attacked_val_embeddings2_normalized)\n",
    "    avg_attacked_val_similarity = np.mean(attacked_val_similarity_matrix)\n",
    "    print(f\"Average attacked validation set similarity between {dsid1} and {dsid2}: {avg_attacked_val_similarity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CASE 4\n",
    "\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from tsai.all import *\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from art.attacks.evasion import (\n",
    "    BasicIterativeMethod, DeepFool, CarliniL2Method, MomentumIterativeMethod,\n",
    "    ElasticNet, AutoProjectedGradientDescent, FastGradientMethod, ZooAttack, BoundaryAttack\n",
    ")\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "\n",
    "torch.use_deterministic_algorithms(False)\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "def initialize_weights(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            torch.nn.init.kaiming_uniform_(param, nonlinearity='relu')  \n",
    "        elif 'bias' in name:\n",
    "            torch.nn.init.zeros_(param)  \n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_filters=64, kernel_size=3, dropout=0.5):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=num_filters, kernel_size=kernel_size)\n",
    "        self.conv2 = nn.Conv1d(in_channels=num_filters, out_channels=num_filters, kernel_size=kernel_size)\n",
    "        self.fc1 = nn.Linear(num_filters, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        initialize_weights(self)\n",
    "\n",
    "    def forward(self, x, return_embeddings=False):\n",
    "        x = x.permute(0, 1, 2)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.adaptive_max_pool1d(x, 1).squeeze(2)\n",
    "        x = self.dropout(x)\n",
    "        final_output = self.fc1(x)\n",
    "\n",
    "        if return_embeddings:\n",
    "            embeddings = x\n",
    "            return final_output, embeddings\n",
    "        else:\n",
    "            return final_output\n",
    "\n",
    "def training_loss(pred, target):\n",
    "    return F.cross_entropy(pred, target)\n",
    "\n",
    "def create_art_classifier(model, input_shape, nb_classes, device):\n",
    "    classifier = PyTorchClassifier(\n",
    "        model=model,\n",
    "        loss=training_loss,\n",
    "        input_shape=input_shape,\n",
    "        nb_classes=nb_classes,\n",
    "        optimizer=torch.optim.Adam(model.parameters(), lr=0.002),\n",
    "        device_type=device.type\n",
    "    )\n",
    "    return classifier\n",
    "\n",
    "def apply_attack(X, classifier, attack_number):\n",
    "    if attack_number == 0:\n",
    "        attack = BasicIterativeMethod(estimator=classifier, eps=0.1)\n",
    "    elif attack_number == 1:\n",
    "        attack = DeepFool(classifier=classifier)\n",
    "    elif attack_number == 2:\n",
    "        attack = CarliniL2Method(classifier=classifier)\n",
    "    elif attack_number == 3:\n",
    "        attack = MomentumIterativeMethod(estimator=classifier, eps=0.1)\n",
    "    elif attack_number == 4:\n",
    "        attack = ElasticNet(classifier=classifier)\n",
    "    elif attack_number == 5:\n",
    "        attack = BoundaryAttack(estimator=classifier, targeted=False)\n",
    "    elif attack_number == 6:\n",
    "        attack = FastGradientMethod(estimator=classifier, eps=0.1)\n",
    "    elif attack_number == 7:\n",
    "        attack = ZooAttack(classifier=classifier, confidence=0.5, targeted=False)\n",
    "    elif attack_number == 8:\n",
    "        attack = AutoProjectedGradientDescent(estimator=classifier, eps=0.1)\n",
    "    else:\n",
    "        return X\n",
    "    return attack.generate(X)\n",
    "\n",
    "def apply_attack_decision(X_split, classifier, attack_decisions):\n",
    "    attacked_pieces = []\n",
    "    for piece, (attack_flag, attack_number) in zip(X_split, attack_decisions):\n",
    "        if attack_flag == 1:\n",
    "            attacked_piece = apply_attack(piece, classifier, attack_number)\n",
    "        else:\n",
    "            attacked_piece = piece\n",
    "        attacked_pieces.append(attacked_piece)\n",
    "    return np.concatenate(attacked_pieces, axis=0)\n",
    "\n",
    "def split_data(X, num_pieces=5):\n",
    "    splits = np.array_split(X, num_pieces)\n",
    "    return splits\n",
    "\n",
    "def generate_attack_decisions(num_pieces=5):\n",
    "    attack_decisions = []\n",
    "    for _ in range(num_pieces):\n",
    "        attack_flag = random.randint(0, 1)\n",
    "        attack_number = random.randint(0, 9) if attack_flag == 1 else None\n",
    "        attack_decisions.append((attack_flag, attack_number))\n",
    "    return attack_decisions\n",
    "\n",
    "def train_and_attack_validation(dsid, hidden_size, device, attack_decisions, batch_size=16, save_path='train_embeddings.npy', attacked_val_save_path='attacked_val_embeddings.npy'):\n",
    "\n",
    "    X_train, y_train, X_test, y_test = get_UCR_data(dsid, return_split=True)\n",
    "    unique_labels = np.unique(y_train)\n",
    "    label_map = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    y_train = np.array([label_map[label] for label in y_train])\n",
    "    y_test = np.array([label_map[label] for label in y_test])\n",
    "\n",
    "    X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train, test_size=0.7, shuffle=False)\n",
    "\n",
    "    print(f\"X_train_split shape: {X_train_split.shape}, X_val_split shape: {X_val_split.shape}\")\n",
    "\n",
    "    tfms = [None, [Categorize()]]\n",
    "    dsets_train = TSDatasets(X_train_split, y_train_split, tfms=tfms, splits=None, inplace=True)\n",
    "    dls_train = TSDataLoaders.from_dsets(dsets_train.train, dsets_train.train, bs=batch_size, shuffle=False, batch_tfms=[TSStandardize()], num_workers=0)\n",
    "\n",
    "    c_in = dls_train.vars\n",
    "    c_out = dls_train.c\n",
    "\n",
    "    model = CustomCNN(c_in, c_out).to(device)\n",
    "    initialize_weights(model)\n",
    "    learn = Learner(dls_train, model, loss_func=training_loss, metrics=accuracy)\n",
    "    learn.fit_one_cycle(25, 0.002)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    classifier = create_art_classifier(model, input_shape=(c_in, X_train_split.shape[2]), nb_classes=c_out, device=device)\n",
    "\n",
    "    X_val_pieces = split_data(X_val_split, num_pieces=5)\n",
    "\n",
    "    attacked_X_val_split = apply_attack_decision(X_val_pieces, classifier, attack_decisions)\n",
    "\n",
    "    def extract_embeddings(X_split, save_path):\n",
    "        X_split_tensor = torch.tensor(X_split).float().to(device)\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(X_split_tensor), batch_size):\n",
    "            batch_X = X_split_tensor[i:i+batch_size]\n",
    "            with torch.no_grad():\n",
    "                _, batch_embeddings = model(batch_X, return_embeddings=True)\n",
    "            all_embeddings.append(batch_embeddings.cpu().numpy())\n",
    "        all_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "        np.save(save_path, all_embeddings)\n",
    "\n",
    "    extract_embeddings(X_train_split, save_path)\n",
    "\n",
    "    extract_embeddings(attacked_X_val_split, attacked_val_save_path)\n",
    "\n",
    "dataset_ids = ['Libras', 'RacketSports', 'NATOPS', 'UWaveGestureLibrary', 'Cricket', 'Ering', 'BasicMotions', 'Epilepsy']\n",
    "hidden_size = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_embeddings_dict = {}\n",
    "attacked_val_embeddings_dict = {}\n",
    "\n",
    "for dsid in dataset_ids:\n",
    "    attack_decisions = generate_attack_decisions(num_pieces=5)\n",
    "    print(f\"Attack decisions for {dsid}: {attack_decisions}\")\n",
    "\n",
    "    train_save_path = f'train_embeddings_{dsid}.npy'\n",
    "    attacked_val_save_path = f'attacked_val_embeddings_{dsid}.npy'\n",
    "    train_and_attack_validation(dsid, hidden_size, device, attack_decisions, batch_size=16, save_path=train_save_path, attacked_val_save_path=attacked_val_save_path)\n",
    "\n",
    "    train_embeddings_dict[dsid] = np.load(train_save_path)\n",
    "    attacked_val_embeddings_dict[dsid] = np.load(attacked_val_save_path)\n",
    "\n",
    "normalized_train_embeddings_dict = {dsid: normalize(train_embeddings_dict[dsid], norm='l2') for dsid in dataset_ids}\n",
    "normalized_attacked_val_embeddings_dict = {dsid: normalize(attacked_val_embeddings_dict[dsid], norm='l2') for dsid in dataset_ids}\n",
    "\n",
    "for dsid1, dsid2 in itertools.combinations(dataset_ids, 2):\n",
    "\n",
    "    train_embeddings1_normalized = normalized_train_embeddings_dict[dsid1]\n",
    "    train_embeddings2_normalized = normalized_train_embeddings_dict[dsid2]\n",
    "    train_similarity_matrix = cosine_similarity(train_embeddings1_normalized, train_embeddings2_normalized)\n",
    "    avg_train_similarity = np.mean(train_similarity_matrix)\n",
    "    print(f\"Average train set similarity between {dsid1} and {dsid2}: {avg_train_similarity}\")\n",
    "\n",
    "    attacked_val_embeddings1_normalized = normalized_attacked_val_embeddings_dict[dsid1]\n",
    "    attacked_val_embeddings2_normalized = normalized_attacked_val_embeddings_dict[dsid2]\n",
    "    attacked_val_similarity_matrix = cosine_similarity(attacked_val_embeddings1_normalized, attacked_val_embeddings2_normalized)\n",
    "    avg_attacked_val_similarity = np.mean(attacked_val_similarity_matrix)\n",
    "    print(f\"Average attacked validation set similarity between {dsid1} and {dsid2}: {avg_attacked_val_similarity}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
